# SentEval

SentEval is a library for evaluating the quality of sentence embeddings as features for a broad and diverse set of "transfer" tasks. It is aimed to ease the study and the development of better general-purpose fixed-size sentence representations (see [InferSent](https://arxiv.org/pdf/1705.02364.pdf)).

## Dependencies

This code is written in python. The dependencies are :

* Python 2.7
* [Pytorch](http://pytorch.org/) >= 0.12
* [NumPy](http://www.numpy.org/) and [SciPy](http://www.scipy.org/)
* [scikit-learn](http://scikit-learn.org/stable/index.html)>="0.18.0"
* [NLTK 3](http://www.nltk.org/)


## Tasks

See [here](https://arxiv.org/pdf/1705.02364.pdf) for a detailed description of the available tasks.
* Binary classification : MR (movie review), CR (product review), SUBJ (subjectivity status), MPQA (opinion-polarity), SST (sentiment analysis)
* Multi-class classification : TREC (question-type classification), SST (fine-grained sentiment analysis)
* Entailment (NLI) and semantic relatedness : [SNLI](https://nlp.stanford.edu/projects/snli/) (entailment), [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) (entailment), [SICK](http://clic.cimec.unitn.it/composes/sick.html) (entailment/relatedness)
* Semantic Textual Similarity : [STSBenchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark#Results), [STS14](http://alt.qcri.org/semeval2014/task10/)
* Paraphrase detection : [MRPC](https://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art))
* Caption-Image retrieval : [COCO](http://mscoco.org/) dataset (with ResNet-101 2048d image embeddings)


## Download datasets
```bash
python data/get_transfer_data.py --data_path your_output_path
```
will automatically download the dataset and preprocess them to "your_output_path".
WARNING : Downloading the [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) dataset requires the "[cabextract](https://www.cabextract.org.uk/#install)" command line (sudo yum install cabextract/sudo apt-get isntall cabextract) to extract the provided Microsoft-specific MSI files. In data/get_transfer_data.py, you can decide to keep or skip the MRPC dataset.

## A small example for average(word2vec/GloVe)

examples/bow.py provides a minimal example for evaluating the quality of the average of word vectors (GloVe) as sentence embeddings.
SentEval requires the user to implement one function : 
    batcher(batch, params)
that takes as input a numpy array of sentences (batch) and some parameters (params), and must return a numpy array of sentence embeddings.

To perform the actual evaluation, first import senteval :
```python
import senteval
```
define a SentEval object : 
```python
se = senteval.SentEval(batcher, prepare, params_senteval)
```
and run the evaluation on a set of transfer tasks : 
```python
transfer_tasks = ['MR', 'MPQA', 'SST', 'TREC', 'SICKRelatedness', 'SICKEntailment', 'MRPC', 'ImageAnnotation']
results = se.eval(transfer_tasks)
```

Run
```bash
python examples/bow.py
```
to evaluate the reproduce the results of avg(GloVe) vectors.

## SentEval parameters
* **task_path** (str) : path to data, generated by data/get_transfer_data.py
* *seed* (int) : random seed for reproducability (default : 1111)
* usepytorch (bool) : use pytorch or scikit learn for logistic regression (default : True)
* batch_size (int) : size of minibatch of text sentences provided to "batcher" (sentences are sorted by length) (note that this is not the batch_size used by pytorch logistic regression, which is fixed).
* verbose (int) : 2->debug, 1->info, 0->warning (default : 2)
* ... and any parameter you want to have access to in "batcher" or "prepare" functions.


## TODO
* Remove "network" parameter in bacher (the user can put it in "params")
* In bow.py, remove the lookup table (get_lut_glove). Create a word_vects dictionnary of word vectors.
* Reduce the number of parameters of senteval.SentEval
* Add SST/multi, add MultiNLI in get_transfer_data
* Add SST/bin on the github. Add COCO dataset on the github


