{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "SAVE_DIR = Path(\"./results\")\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all output LayerNorms weights and biases\n",
    "values = {\"weights\": [], \"biases\": []}\n",
    "weights, biases = [], []\n",
    "for roberta_layer in model.encoder.layer:\n",
    "    the_layer = roberta_layer.output.LayerNorm\n",
    "    values[\"weights\"].append(the_layer.weight)\n",
    "    values[\"biases\"].append(the_layer.bias)\n",
    "values[\"weights\"] = torch.stack(values[\"weights\"])\n",
    "values[\"biases\"] = torch.stack(values[\"biases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std of gathered values\n",
    "statistics = {\n",
    "    key: {\n",
    "        \"mean\": val.mean(),\n",
    "        \"std\": val.std(),\n",
    "    }\n",
    "    for key, val in values.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outlier dimensions and compute distances from the mean\n",
    "\n",
    "# # set hyperparameters for getting outlier dimensions\n",
    "sigma_factor = 2\n",
    "out_count_thres = 4\n",
    "\n",
    "# # get outlier dimensions\n",
    "param_outliers = {}\n",
    "param_distance_from_mean = {}\n",
    "for param in values:\n",
    "    distance_from_mean = (values[param] - statistics[param][\"mean\"]).abs()\n",
    "    std = statistics[param][\"std\"]\n",
    "    out_count = (distance_from_mean > (sigma_factor * std)).int().sum(dim=0)\n",
    "    outliers_mask = out_count >= out_count_thres\n",
    "\n",
    "    outliers_idcs = torch.arange(len(outliers_mask))[outliers_mask]\n",
    "    param_outliers[param] = list(outliers_idcs.numpy())\n",
    "    param_distance_from_mean[param] = distance_from_mean.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 45\n",
      "biases 19\n"
     ]
    }
   ],
   "source": [
    "# number of ods for weight and biases\n",
    "for param in param_outliers:\n",
    "    print(param, len(param_outliers[param]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(intersection) = 17\n",
      "intersection = {97, 453, 551, 361, 330, 588, 77, 749, 494, 240, 496, 82, 498, 217, 219, 61, 731}\n"
     ]
    }
   ],
   "source": [
    "# intersection of ods for each param -> it give us real ods we are going to work with\n",
    "intersection = set(param_outliers[\"weights\"]).intersection(set(param_outliers[\"biases\"]))\n",
    "print(f\"{len(intersection) = }\")\n",
    "print(f\"{intersection = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier_dimensions = [61, 77, 82, 97, 217, 219, 240, 330, 361, 453, 494, 496, 498, 551, 588, 731, 749]\n"
     ]
    }
   ],
   "source": [
    "outlier_dimensions = sorted(list(intersection))\n",
    "print(f\"{outlier_dimensions = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_DIR / \"outlier_dimensions.npy\", \"wb\") as file:\n",
    "    np.save(file, np.asarray(outlier_dimensions))\n",
    "\n",
    "\n",
    "for param in param_distance_from_mean:\n",
    "    with open(SAVE_DIR / f\"dist_from_mean_{param}.npy\", \"wb\") as file:\n",
    "        np.save(file, np.asarray(param_distance_from_mean[param].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_DIR / \"outlier_dimensions.npy\", \"rb\") as file:\n",
    "    r = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
